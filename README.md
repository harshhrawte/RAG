🤖 AlgoAssist: DSA RAG Chatbot
AlgoAssist is an intelligent Retrieval-Augmented Generation (RAG) chatbot designed to help students and developers learn and explore Data Structures and Algorithms (DSA). It uses a combination of LangChain, ChromaDB, LLaMA3, and Streamlit to provide accurate, context-aware answers extracted from a DSA book.

🚀 Features
📘 PDF Knowledge Base: Ingests and indexes a DSA textbook using ChromaDB and sentence-transformer embeddings.

🔍 Retrieval-Augmented Generation: Retrieves relevant chunks before generating responses using LLaMA3.

💬 Natural Chat Interface: Built with Streamlit for an interactive and user-friendly experience.

⚙️ Efficient Query Processing: Uses sentence-transformers for semantic search and LLaMA3-8B for generation.

📁 Project Structure

LLM-RAG/
├── app/
│   ├── __init__.py
│   ├── chatbot.py           # Core RAG logic
│   ├── chroma_data.py       # Builds ChromaDB from PDF
│   ├── llm.py               # Loads LLM, retriever, and prompt
├── chroma_db/               # Persistent vector store
│   ├── chroma.sqlite3
│   └── <auto-generated UUID folder>
├── data/
│   └── DSA BOOK.pdf         # Source document (DSA book)
├── interface/
│   ├── __init__.py
│   └── streamlit.py         # Streamlit frontend
├── llm-rag-env/             # Your virtual environment (not version controlled)
├── config.py                # API keys and constants
├── requirements.txt         # Python dependencies
├── run.sh                   # Shell script to start app
├── Dockerfile               # Docker support
├── .env                     # Environment variables (not version controlled)
├── .gitignore
├── .dockerignore
└── README.md                # Project documentation

🛠️ Setup Instructions
1. Clone the Repository
 
git clone https://github.com/harshhrawte/LLM-RAG.git
cd LLM-RAG

2. Create a Virtual Environment

python -m venv env
source env/bin/activate   # On Windows: env\Scripts\activate

3. Install Dependencies

pip install -r requirements.txt

4. Set Up API Keys
Create a config.py file in the root directory with the following content:

GROQ_API_KEY = "your-groq-api-key"
HF_TOKEN = "your-huggingface-token"

✅ Note: You’ll need a Groq API key and a HuggingFace token to use the LLaMA3 model and embeddings.

I have added the chroma_db as well so that bu just entering the api , one can directly run 

 streamlit run streamlit.py

 Visit http://localhost:8501 in your browser to start chatting with AlgoAssist!

 Some Screenshots of the output:

 