{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2598b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 4 major .py codes which contributes the whole RAG Pipeline \n",
    "\n",
    "\n",
    "#this is streamlit code the main code \n",
    "\n",
    "# streamlit.py\n",
    "# Streamlit UI for AlgoAssist - An Intelligent DSA Learning Companion\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "# Make sure we can import from the 'app' folder\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "from app.chatbot import answer_query  # This handles the logic of answering user queries\n",
    "\n",
    "# Basic setup for the page appearance\n",
    "st.set_page_config(\n",
    "    page_title=\"AlgoAssist\",    # What shows on the browser tab\n",
    "    page_icon=\"ðŸ¤–\",             # A fun little robot icon\n",
    "    layout=\"centered\"           # Keep everything centered for better UX\n",
    ")\n",
    "\n",
    "# A bit of custom CSS to make the page look nice\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        text-align: center;\n",
    "        color: #4CAF50;\n",
    "    }\n",
    "    .subheader {\n",
    "        text-align: center;\n",
    "        font-size: 1.2em;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Main title and subtitle right at the top\n",
    "st.markdown(\"<h1 class='main-header'>AlgoAssist ðŸ¤–</h1>\", unsafe_allow_html=True)\n",
    "st.markdown(\"<p class='subheader'>Your intelligent DSA learning companion</p>\", unsafe_allow_html=True)\n",
    "\n",
    "# If it's the user's first time here, we start a fresh chat\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Loop through and show previous messages (both user and assistant)\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "# On the left: a list of clickable sample questions to get started quickly\n",
    "with st.sidebar:\n",
    "    st.header(\"Sample Questions\")\n",
    "    sample_questions = [\n",
    "        \"What is the time complexity of quicksort?\",\n",
    "        \"Explain binary search trees\",\n",
    "        \"How does a hash table work?\",\n",
    "        \"Difference between BFS and DFS\",\n",
    "        \"Explain dynamic programming\"\n",
    "    ]\n",
    "\n",
    "    # If user clicks one, we simulate sending it as their question\n",
    "    for question in sample_questions:\n",
    "        if st.button(question):\n",
    "            st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                answer = answer_query(question)\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            st.rerun()  # Refresh to update the chat interface with new messages\n",
    "\n",
    "# Main input box where the user types their custom query\n",
    "query = st.chat_input(\"Ask about Data Structures & Algorithms\")\n",
    "\n",
    "if query:\n",
    "    # Store the userâ€™s question in chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    # Show the question in the chat bubble\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(query)\n",
    "\n",
    "    # Generate the answer and show it\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Generating answer...\"):\n",
    "            answer = answer_query(query)\n",
    "            st.write(answer)\n",
    "\n",
    "    # Save the assistant's reply to chat history too\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is llm code \n",
    "# Backend logic for AlgoAssist - handles LLM, embeddings, and prompt creation\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from chromadb import PersistentClient\n",
    "from config import GROQ_API_KEY, HF_TOKEN  # API keys for Groq and Hugging Face\n",
    "import streamlit as st\n",
    "\n",
    "# Cache the LLM instance so it doesn't reload every time the app runs\n",
    "@st.cache_resource\n",
    "def get_llm():\n",
    "    return ChatGroq(\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        model_name=\"Llama3-8b-8192\",  # Using Groq's LLaMA3 model\n",
    "        temperature=0.5,              # Balanced creativity\n",
    "        max_tokens=1024,              # Max length of response\n",
    "        max_retries=3,                # Retry if API fails\n",
    "        request_timeout=30            # Timeout in seconds\n",
    "    )\n",
    "\n",
    "# This sets up the vector DB retriever using Chroma and HuggingFace embeddings\n",
    "def get_retriever(persist_dir=\"chroma_db\"):\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    # Connect to Chroma's persistent storage\n",
    "    client = PersistentClient(path=persist_dir)\n",
    "    collection = client.get_collection(name=\"langchain\")\n",
    "\n",
    "    # Return both collection and embeddings so they can be used together\n",
    "    return collection, embeddings\n",
    "\n",
    "# This builds the prompt template that guides the assistant's behavior\n",
    "def create_prompt():\n",
    "    return ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are a helpful assistant specialized in Data Structures and Algorithms.\n",
    "Use the following context to answer the question.\n",
    "If the context is not relevant, provide an accurate answer based on your knowledge.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.llm import get_llm, get_retriever, create_prompt\n",
    "\n",
    "#from llm to chatbot \n",
    "def answer_query(query):\n",
    "    llm = get_llm()\n",
    "    collection, embeddings = get_retriever()\n",
    "    prompt = create_prompt()\n",
    "\n",
    "    embedding_vector = embeddings.embed_query(query)\n",
    "    results = collection.query(query_embeddings=[embedding_vector], n_results=3)\n",
    "\n",
    "    context = \"\\n\".join(doc for doc in results[\"documents\"][0])\n",
    "    response = llm.invoke(prompt.format(context=context, query=query))\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd745f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_chroma.py chroma vector database \n",
    "# Script to build Chroma vector database from a DSA PDF using HuggingFace embeddings\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from config import HF_TOKEN  # Hugging Face token (in case it's needed)\n",
    "\n",
    "# Add the root directory to sys.path to import config.py and access project files easily\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "def build_chroma_db(pdf_paths, persist_dir=\"chroma_db\"):\n",
    "    \"\"\"\n",
    "    Loads PDFs, splits the text, and builds a Chroma vector store\n",
    "    using HuggingFace sentence embeddings.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "\n",
    "    # Load documents from all provided PDF paths\n",
    "    for path in pdf_paths:\n",
    "        loader = PyPDFLoader(path)\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    # Split documents into manageable chunks for embedding\n",
    "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = splitter.split_documents(docs)\n",
    "\n",
    "    # Use HuggingFace transformer model to generate embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    # Create and persist the Chroma vector database\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "    vectordb.persist()\n",
    "\n",
    "    print(f\"âœ… Chroma DB built successfully with {len(texts)} document chunks.\")\n",
    "\n",
    "\n",
    "# Run this script directly to build the DB\n",
    "if __name__ == \"__main__\":\n",
    "    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    \n",
    "    # You can add multiple PDFs in this list if needed\n",
    "    pdf_paths = [os.path.join(project_root, \"data\", \"DSA BOOK.pdf\")]\n",
    "\n",
    "    build_chroma_db(pdf_paths)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
